# leomax_tokenizer

这个仓库是对 fast_tokenizer 的学习

## 分词算法

### WordPiece
* 测试分词词典

```shell
wget https://bj.bcebos.com/paddlenlp/models/transformers/ernie/vocab.txt
```